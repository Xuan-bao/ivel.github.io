<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>神经网络初始化问题 | Ivel</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="log： 2024&#x2F;11&#x2F;22：部分图片 1234import numpyfrom numpy import randomimport matplotlib.pyplot as pltimport math 初始化权重预实验：【输入】常见做法：将输入值缩放到均值为0，标准差为1的正态分布中。 【权值】初始化：若采用相同的标准正态分布$N(0,1)$ 12345x&#x3D;random.normal(loc">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络初始化问题">
<meta property="og:url" content="https://ivel-li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/index.html">
<meta property="og:site_name" content="Ivel">
<meta property="og:description" content="log： 2024&#x2F;11&#x2F;22：部分图片 1234import numpyfrom numpy import randomimport matplotlib.pyplot as pltimport math 初始化权重预实验：【输入】常见做法：将输入值缩放到均值为0，标准差为1的正态分布中。 【权值】初始化：若采用相同的标准正态分布$N(0,1)$ 12345x&#x3D;random.normal(loc">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://ivel-li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/tanhx&amp;softsign.png">
<meta property="og:image" content="https://ivel-li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/Xavier%E7%9A%84%E6%8F%90%E5%8D%87.png">
<meta property="og:image" content="https://ivel-li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/He%E5%88%9D%E5%A7%8B%E5%8C%96%E7%AD%96%E7%95%A5.png">
<meta property="og:image" content="https://ivel-li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/%E5%AF%B9%E6%AF%94x&amp;h.png">
<meta property="article:published_time" content="2024-11-21T15:07:34.000Z">
<meta property="article:modified_time" content="2024-11-22T13:44:40.803Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ivel-li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/tanhx&amp;softsign.png">
  
    <link rel="alternate" href="https://ivel-li.github.io/atom.xml" title="Ivel" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="https://ivel-li.github.io/favicon.ico">
  
  
  
<link rel="stylesheet" href="https://ivel-li.github.io/css/style.css">

  
    
<link rel="stylesheet" href="https://ivel-li.github.io/fancybox/jquery.fancybox.min.css">

  
  
  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>


<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="https://ivel-li.github.io/" id="logo">Ivel</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="https://ivel-li.github.io/" id="subtitle">Greeting, welcome to my blog</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="https://ivel-li.github.io/">Home</a>
        
          <a class="main-nav-link" href="https://ivel-li.github.io/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="https://ivel-li.github.io/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ivel-Li.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-初始化问题" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="https://ivel-li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/" class="article-date">
  <time class="dt-published" datetime="2024-11-21T15:07:34.000Z" itemprop="datePublished">2024-11-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="https://ivel-li.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>►<a class="article-category-link" href="https://ivel-li.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E4%B8%8Etrick/">网络基础理论与trick</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      神经网络初始化问题
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!-- 文章目录 -->
        
        <!-- Table of Contents -->
        <p><strong>log：</strong></p>
<p>2024/11/22：部分图片</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></table></figure>
<h2 id="初始化权重预实验："><a href="#初始化权重预实验：" class="headerlink" title="初始化权重预实验："></a>初始化权重预实验：</h2><p>【输入】常见做法：将输入值缩放到均值为0，标准差为1的正态分布中。</p>
<p>【权值】初始化：若采用相同的标准正态分布$N(0,1)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>,<span class="number">512</span>))<span class="comment">#a对应每层神经网络中的W：512*512</span></span><br><span class="line">    x=a@x</span><br><span class="line">x.mean(), x.std()<span class="comment">###向量.mean()输出所有元素的均值；.std()输出所有元素标准差</span></span><br></pre></td></tr></table></figure>
<pre><code>(7.943177084708433e+133, 2.849247295078971e+135)
</code></pre><p>如上可观察到，在100层的神经网络中，对初值和权重取$N(0,1)$分布，最终输出极大。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">0.01</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为0.01的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=random.normal(loc=<span class="number">0</span>, scale=<span class="number">0.01</span>, size=(<span class="number">512</span>,<span class="number">512</span>))</span><br><span class="line">    x=a@x</span><br><span class="line">x.mean(), x.std()</span><br></pre></td></tr></table></figure>
<pre><code>(3.0804500580946894e-69, 2.214484543669681e-67)
</code></pre><p>如上可观察到，在100层的神经网络中，对初值和权重取$N(0,0.01)$分布，最终输出极小。（ps: 在取$N(0,0.1)$时输出也很大）</p>
<p><strong><em>即：初始化权重过大过小均会影响学习效果</em></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
<p>上式中y[i]表达式可描述一层神经元的映射关系</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mean, var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="comment">#i的大小与神经网络层数无关，意义只在于i够大时结果更精确</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>))</span><br><span class="line">    a=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>,<span class="number">512</span>))</span><br><span class="line">    y=a@x</span><br><span class="line">    mean+=y.mean().item()<span class="comment">#item()函数取值比索引取值精度更高</span></span><br><span class="line">    var+=numpy.multiply(y,y).mean().item()</span><br><span class="line">mean/<span class="number">1000</span>,math.sqrt(var/<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(-0.03692881605069143, 22.68767143694423)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">math.sqrt(<span class="number">512</span>)</span><br></pre></td></tr></table></figure>
<pre><code>22.627416997969522
</code></pre><p><strong>此时我们注意到Y的标准差$\frac{\Sigma_j^m mean(y_i^2)}{m}$【y平均值为0】, 其逼近x维数（神经元个数）的平方根。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#它教会了我如何计算均值为0的一组数据的标准差（通过D(X)=E(X^2))。</span></span><br><span class="line">mean, var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>))</span><br><span class="line">    a=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>,<span class="number">512</span>))</span><br><span class="line">    y=a@x</span><br><span class="line">    mean+=y.mean().item()<span class="comment">#item()函数取值比索引取值精度更高（不知道为什么）</span></span><br><span class="line">    var+=numpy.multiply(y,y).mean().item()</span><br><span class="line">mean/<span class="number">1000</span>,math.sqrt(var/<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(-0.02495151144918943, 22.6154885506345)
</code></pre><p>一些数学推导：</p>
<font color="grey" size="1">

方差$D(X)=E（[X-E(X)]^2）$

$D(XY)=E([XY-E(XY)]^2)$

又当X,Y相互独立时，有$E(XY)=E(X)E(Y)$

故有$D(XY)=E(X^2Y^2+E^2(XY)-2XYE(XY))$

当X，Y满足正态分布$N(0,1)$，有$D(XY)=E(X^2Y^2)=E(X^2)E(Y^2)=D(X)D(Y)$

$D(X+Y)=E（[X+Y-E(X+Y)]^2）=E（[X+Y]^2)=E(X^2)+E(Y^2)+2E(XY)=E(X^2)+E(Y^2)=D(X)+D(Y)$
</font>

<p>有了上述结论，对$N(0,1)$分布的$X,Y$而言，有$D(XY)=D(X)D(Y),D(X+Y)=D(X)+D(Y)$。</p>
<p>因此上述代码中，Y的每个元素方差为512。</p>
<p>上面代码块中var中右式实际上为$E（Y^2)$，当i迭代次数够大求和平均等价于$D(Y)$。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mean, var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>))</span><br><span class="line">    a=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>,<span class="number">512</span>))*math.sqrt(<span class="number">1.</span>/<span class="number">512</span>)<span class="comment">#D(cX)=c^2 D(X)</span></span><br><span class="line">    y=a@x</span><br><span class="line">    mean+=y.mean().item()</span><br><span class="line">    var+=numpy.multiply(y,y).mean().item()</span><br><span class="line">mean/<span class="number">1000</span>,math.sqrt(var/<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(-0.0009292457617292619, 0.9998508585319182)
</code></pre><p> <strong>由上可见，把$N(0,1)$的初始化权值缩放至$1/\sqrt{n}$后输出层不再出现爆炸。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>,<span class="number">512</span>))*math.sqrt(<span class="number">1.</span>/<span class="number">512</span>)</span><br><span class="line">    x=a@x</span><br><span class="line">x.mean(), x.std()</span><br></pre></td></tr></table></figure>
<pre><code>(-0.013509481923589781, 0.669791021961652)
</code></pre><p><strong>由上可见，把$N(0,1)$的初始化权值缩放至$1/\sqrt{n}$后输出层（均值和方差）不再出现爆炸。即便在100层后亦是如此。</strong></p>
<h2 id="当我们考虑激活函数"><a href="#当我们考虑激活函数" class="headerlink" title="当我们考虑激活函数"></a>当我们考虑激活函数</h2><p>饱和激活函数F：$lim_{x-&gt;\infty}F’-&gt;0$</p>
<p>非饱和激活函数F：$lim_{x-&gt;\infty}F’不趋于0$</p>
<p>典型的<strong>饱和函数</strong>有Sigmoid，Tanh函数。</p>
<p>不满足饱和函数条件的函数则称为<strong>非饱和激活函数</strong>。</p>
<p>ReLU及其变体则是“非饱和激活函数”。</p>
<p>使用“非饱和激活函数”的优势在于两点：</p>
<ol>
<li>“非饱和激活函数”能解决所谓的“梯度消失”问题。</li>
</ol>
<ol>
<li>它能加快收敛速度。</li>
</ol>
<h3 id="Xavier初始化（适用于饱和激活函数）"><a href="#Xavier初始化（适用于饱和激活函数）" class="headerlink" title="Xavier初始化（适用于饱和激活函数）"></a>Xavier初始化（适用于饱和激活函数）</h3><p><img src="https://ivel-Li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/tanhx&amp;softsign.png" alt="image"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>,<span class="number">512</span>))*math.sqrt(<span class="number">1.</span>/<span class="number">512</span>)</span><br><span class="line">    x=numpy.array([math.tanh(c)<span class="keyword">for</span> c <span class="keyword">in</span> a@x])</span><br><span class="line">x.mean(), x.std()</span><br></pre></td></tr></table></figure>
<pre><code>(0.0008083661917762789, 0.05757114952184833)
</code></pre><p>可以观察到，在一百层神经网络后，x的均值与标准差值仍处于有效范围，激活未完全消失。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=numpy.random.uniform(low=-<span class="number">1</span>,high=<span class="number">1</span>,size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1/3的均匀分布（从-1到1取值），512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>,<span class="number">512</span>))*math.sqrt(<span class="number">1.</span>/<span class="number">512</span>)</span><br><span class="line">    x=numpy.array([math.tanh(c)<span class="keyword">for</span> c <span class="keyword">in</span> a@x])</span><br><span class="line">x.mean(), x.std()</span><br></pre></td></tr></table></figure>
<pre><code>(0.0013361727805947863, 0.0572257326796936)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment">#权值取均值为0，方差为1/3的均匀分布（从-1到1取值），512*512个数据*1/n</span></span><br><span class="line">    a=numpy.random.uniform(low=-<span class="number">1</span>,high=<span class="number">1</span>,size=(<span class="number">512</span>,<span class="number">512</span>))*math.sqrt(<span class="number">1.</span>/<span class="number">512</span>)</span><br><span class="line">    x=numpy.array([math.tanh(c)<span class="keyword">for</span> c <span class="keyword">in</span> a@x])</span><br><span class="line">x.mean(), x.std()</span><br></pre></td></tr></table></figure>
<pre><code>(6.753492971342052e-26, 1.2087455023485755e-24)
</code></pre><p><strong>注意到两种算法区别在于，上述x的1/3方差只出现一次，大量迭代后不会对后续结果造成影响，后者令a的权值初始化为1/3，a会出现100次，导致了最终激活梯度无穷小。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment">#权值取均值为0，方差为1的均匀分布（从-sqrt3到sqrt3取值），512*512个数据*1/n</span></span><br><span class="line">    a=numpy.random.uniform(low=-<span class="number">1.73205081</span>,high=<span class="number">1.73205081</span>,size=(<span class="number">512</span>,<span class="number">512</span>))*math.sqrt(<span class="number">1.</span>/<span class="number">512</span>)</span><br><span class="line">    x=numpy.array([math.tanh(c)<span class="keyword">for</span> c <span class="keyword">in</span> a@x])</span><br><span class="line">x.mean(), x.std()</span><br></pre></td></tr></table></figure>
<pre><code>(0.0018487857398481779, 0.05713394734193917)
</code></pre><p><strong>如上所示，当a取均匀分布但方差为1时，100次迭代后x的均值与标准差值仍处于有效范围，激活未完全消失。</strong></p>
<p><strong>因此笔者猜想，或者核心在于使a@x（或者说 $W^TX$ ）的方差为1</strong></p>
<p>对于<strong>Xavier初始化</strong>方法，其将<strong>每层的权重</strong>设置为:</p>
<p>$\pm\frac{\sqrt{6}}{\sqrt{n_i+n_{i+1}}}$</p>
<p>上式中$n_i$为传入该层的连接的数量（扇入），$n_{i+1}$为传出该层的连接的数量，也被称为（扇出）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">xavier</span>(<span class="params">m,h</span>):</span><br><span class="line">    <span class="keyword">return</span> numpy.random.uniform(low=-<span class="number">1</span>,high=<span class="number">1</span>,size=(m,h))*math.sqrt(<span class="number">6.</span>/(m+h))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">xbvier</span>(<span class="params">m,h</span>):</span><br><span class="line">    <span class="keyword">return</span> numpy.random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(m,h))*math.sqrt(<span class="number">6.</span>/(m+h))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#xavier初始化</span></span><br><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=xavier(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x=numpy.array([math.tanh(c)<span class="keyword">for</span> c <span class="keyword">in</span> a@x])</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#xavier初始化，但是权重并非U（-1，1）而是N（0，1）分布</span></span><br><span class="line">x1=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=xbvier(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x1=numpy.array([math.tanh(c)<span class="keyword">for</span> c <span class="keyword">in</span> a@x1])</span><br><span class="line"><span class="built_in">print</span>(x1.mean(), x1.std())</span><br><span class="line"><span class="comment">#凑方差为1初始化，权重U（-1，1）分布</span></span><br><span class="line">x2=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=numpy.random.uniform(low=-<span class="number">1</span>,high=<span class="number">1</span>,size=(<span class="number">512</span>,<span class="number">512</span>))*math.sqrt(<span class="number">3.</span>/<span class="number">512</span>)</span><br><span class="line">    x2=numpy.array([math.tanh(c)<span class="keyword">for</span> c <span class="keyword">in</span> a@x2])</span><br><span class="line"><span class="built_in">print</span>(x2.mean(), x2.std())</span><br><span class="line"></span><br><span class="line"><span class="comment">#以下为无激活函数的情况</span></span><br><span class="line"><span class="comment">#xavier初始化</span></span><br><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=xavier(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x=a@x</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"><span class="comment">#xavier初始化，但是权重并非U（-1，1）而是N（0，1）分布</span></span><br><span class="line">x1=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=xbvier(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x1=a@x</span><br><span class="line"><span class="built_in">print</span>(x1.mean(), x1.std())</span><br><span class="line"><span class="comment">#凑方差为1初始化，权重U（-1，1）分布</span></span><br><span class="line">x2=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=numpy.random.uniform(low=-<span class="number">1</span>,high=<span class="number">1</span>,size=(<span class="number">512</span>,<span class="number">512</span>))*math.sqrt(<span class="number">3.</span>/<span class="number">512</span>)</span><br><span class="line">    x2=numpy.array([math.tanh(c)<span class="keyword">for</span> c <span class="keyword">in</span> a@x2])</span><br><span class="line"><span class="built_in">print</span>(x2.mean(), x2.std())</span><br></pre></td></tr></table></figure>
<pre><code>-0.0011354134828090012 0.06322324251617281
-0.04347840333790784 0.6898032256032508
-0.0025500434343385574 0.04863383928260507
-0.016140350889364705 0.7301846105693588
-0.08559922112658865 1.341156944469779
0.000651830089541752 0.09598924410829489
</code></pre><p>此时对于权值初始化$U(-1,1)$的情况，Xavier初始化表现良好。<br>数学上很简单，此时$\pm\frac{\sqrt{6}}{\sqrt{n_i+n_{i+1}}}=\pm\frac{\sqrt{3<em>2}}{\sqrt{512</em>2}}$，即$\frac{\sqrt{3}}{\sqrt{512}}$，等价于令方差为1/3，均值为0，从（-1，1）均匀取值，512个数据的X对应的a@x（或者说 𝑊𝑇𝑋 ）的方差为1。</p>
<p><strong>以上内容对有无激活函数情况下：xavier初始化对权重U（-1，1）分布、N（0，1）分布、凑方差1初始化进行了比较。</strong></p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><ol>
<li>有激活函数时方差传递效果比无激活函数时较弱，推测原因在于tanhx这一激活函数本身特性限制了方差传递</li>
<li>xavier初始化对权重N（0，1）分布相较于U（-1，1）分布，理论上每层方差变化分别为3、1，结果上确实可以观测到最终权重N（0，1）分布引出的x1的方差较大，但并未在100层传递下出现爆炸（10000层也不会）。<br><strong>方差凑1法仍需继续讨论</strong></li>
</ol>
<p><strong>Xavier方法的适用条件：</strong></p>
<ol>
<li>权重U（-1，1）分布。<br>2.激活函数为饱和激活函数。</li>
<li>网络层为前馈全连接神经网络层。（应该吧）</li>
</ol>
<p><img src="https://ivel-Li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/Xavier的提升.png" alt="image"></p>
<p>为了说明这一点，Glorot和Bengio证明，使用Xavier初始化的网络在CIFAR-10图像分类任务上实现了更快的收敛速度和更高的准确性。</p>
<p><strong>Kaiming初始化（he初始化）</strong></p>
<p>从概念上讲，当使用关于0对称且在[-1,1]内部有输出(如softsign和tanh)的激活函数时，我们希望每个层的激活输出的平均值为0，平均标准偏差为1，这是有意义的,这也正是Xavier所支持的。</p>
<p>但是如果我们使用ReLU激活函数呢？以同样的方式缩放随机初始权重值是否仍然有意义？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)): </span><br><span class="line">        temp=numpy.array([x[i],<span class="number">0</span>])</span><br><span class="line">        x[i]=temp.<span class="built_in">max</span>()</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">mean, var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>))</span><br><span class="line">    a=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>,<span class="number">512</span>))</span><br><span class="line">    y=relu(a@x)</span><br><span class="line">    mean+=y.mean().item()</span><br><span class="line">    var+=numpy.multiply(y,y).mean().item()</span><br><span class="line">mean/<span class="number">1000</span>,math.sqrt(var/<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(9.048598009365435, 16.032879062442056)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">math.sqrt(<span class="number">512</span>/<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>16.0
</code></pre><p>我们注意到，RELU激活时y的标准差可直接计算，$D(Y)=\frac{\sqrt{512}}{\sqrt{2}}$,<strong>即激活函数RELU将使标准差缩小$\frac{1}{\sqrt{2}}$。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mean, var=<span class="number">0.</span>,<span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>))</span><br><span class="line">    a=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=(<span class="number">512</span>,<span class="number">512</span>))*math.sqrt(<span class="number">2</span>/<span class="number">512</span>)</span><br><span class="line">    y=relu(a@x)</span><br><span class="line">    mean+=y.mean().item()</span><br><span class="line">    var+=numpy.multiply(y,y).mean().item()</span><br><span class="line">mean/<span class="number">1000</span>,math.sqrt(var/<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p>使每个初始化权重乘$\frac{\sqrt{2}}{\sqrt{512}}$，保持激活层标准差在1左右。</p>
<p><img src="https://ivel-Li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/He初始化策略.png" alt="image"></p>
<p><strong>个人理解：$\sqrt{n}$代表正态分布权值矩阵维度的影响，$\sqrt{2}$代表Relu激活函数的影响</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 表示扇入数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kaiming</span>(<span class="params">m,h</span>):</span><br><span class="line">    <span class="keyword">return</span> numpy.random.normal(loc=<span class="number">0</span>,scale=<span class="number">1</span>,size=(m,h))*math.sqrt(<span class="number">2.</span>/h)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kaiming1</span>(<span class="params">m,h</span>):</span><br><span class="line">    <span class="keyword">return</span> numpy.random.normal(loc=<span class="number">0</span>,scale=<span class="number">2</span>,size=(m,h))*math.sqrt(<span class="number">2.</span>/h)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kaiming2</span>(<span class="params">m,h</span>):</span><br><span class="line">    <span class="keyword">return</span> numpy.random.uniform(low=-<span class="number">1</span>,high=<span class="number">1</span>,size=(m,h))*math.sqrt(<span class="number">2.</span>/h)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=kaiming(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x=relu(a@x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=xavier(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x=relu(a@x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"></span><br><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=kaiming1(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x=relu(a@x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x=random.normal(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=<span class="number">512</span>)<span class="comment">#均值为0，方差为1的正态分布，512个数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    a=kaiming2(<span class="number">512</span>,<span class="number">512</span>)</span><br><span class="line">    x=relu(a@x)</span><br><span class="line"><span class="built_in">print</span>(x.mean(), x.std())</span><br></pre></td></tr></table></figure>
<p>可以观察到，对于ReLU激活函数,使用Xavier初始化将导致激活输出在100层几乎消失。</p>
<p>对于不能使初始权重方差为1的情况，如$U(-1,1)、N(0,2)$，如此方法表现并不佳，即<strong>方差凑1依旧十分重要</strong></p>
<p><img src="https://ivel-Li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/对比x&amp;h.png" alt="image"></p>
<p><strong>Kaiming方法的适用条件：</strong></p>
<ol>
<li>权重N（0，1）分布（方差为1）。<br>2.激活函数为非饱和激活函数ReLU。</li>
<li>网络层为前馈全连接神经网络层。（应该吧）</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li><p>核心：方差凑1</p>
</li>
<li><p>激活函数（复杂的激活函数无法计算）：</p>
</li>
</ol>
<p>&emsp;&emsp;<strong>2.1 tanh()&amp;Sigmoid() =&gt; Xavier初始化方法：权重U（-1，1）分布$*\frac{\sqrt{6}}{\sqrt{n_i+n_{i+1}}}$</strong></p>
<p>&emsp;&emsp;<strong>2.2 ReLU() =&gt; kaiming初始化方法：权重N（-1，1）分布$*\frac{\sqrt{2}}{\sqrt{n}}$</strong></p>
<p><strong>此外上述方法理论上在前馈全连接层较为适用</strong></p>
<p>实用语法：</p>
<ol>
<li>x.item():取出较x直接索引、精度更高的值</li>
<li>numpy.random.normal(loc=0,scale=1,size=(m,h)): m*h矩阵， 元素为N(0,1)正态分布。</li>
<li>numpy.random.uniform(low=-1,high=1,size=(m,h)): m*h矩阵， 元素为U(-1,1)均匀分布。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ivel-li.github.io/2024/11/21/%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/" data-id="cm3x4d3j0000dg8uu0qynczg9" data-title="神经网络初始化问题" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="https://ivel-li.github.io/2024/11/21/%E5%A4%A7%E7%AC%A8%E7%8C%AA%E5%90%8D%E5%8D%95/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          大笨猪名单
        
      </div>
    </a>
  
  
    <a href="https://ivel-li.github.io/2024/11/21/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
  
    <script>
      var assetsString = '<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">'
      var pHead = document.getElementsByTagName('head')[0];
      pHead.innerHTML = pHead.innerHTML + assetsString;
    </script>
  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="https://ivel-li.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="https://ivel-li.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E4%B8%8Etrick/">网络基础理论与trick</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="https://ivel-li.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E7%BD%91%E7%BB%9C/">计算机科学网络</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="https://ivel-li.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E7%BD%91%E7%BB%9C/%E5%AE%9E%E7%94%A8%E7%9F%A5%E8%AF%86%E4%B8%8E%E5%B7%A5%E5%85%B7/">实用知识与工具</a></li></ul></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="https://ivel-li.github.io/archives/2024/11/">November 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="https://ivel-li.github.io/2024/11/25/%E7%AB%AF%E5%8F%A3%E6%98%AF%E4%BB%80%E4%B9%88/">端口是什么</a>
          </li>
        
          <li>
            <a href="https://ivel-li.github.io/2024/11/25/IP%E6%98%AF%E4%BB%80%E4%B9%88/">IP是什么</a>
          </li>
        
          <li>
            <a href="https://ivel-li.github.io/2024/11/25/Github%E4%BD%BF%E7%94%A8%E4%B8%8E%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86%EF%BC%88Git%E4%B8%8E%E5%BB%BA%E7%AB%8B%E8%87%AA%E5%B7%B1%E7%9A%84Github%E4%BB%93%E5%BA%93%EF%BC%89/">Github使用与背景知识（Git与建立自己的Github仓库）</a>
          </li>
        
          <li>
            <a href="https://ivel-li.github.io/2024/11/25/ssh%E6%A6%82%E8%BF%B0/">ssh概述</a>
          </li>
        
          <li>
            <a href="https://ivel-li.github.io/2024/11/21/%E5%A4%A7%E7%AC%A8%E7%8C%AA%E5%90%8D%E5%8D%95/">大笨猪名单</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="https://ivel-li.github.io/" class="mobile-nav-link">Home</a>
  
    <a href="https://ivel-li.github.io/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="https://ivel-li.github.io/js/jquery-3.6.4.min.js"></script>



  
<script src="https://ivel-li.github.io/fancybox/jquery.fancybox.min.js"></script>




<script src="https://ivel-li.github.io/js/script.js"></script>





  </div>
<!-- hexo injector body_end start --><script data-pjax src="https://registry.npmmirror.com/oh-my-live2d/latest/files"></script><script>const oml2d = OML2D.loadOml2d({libraryUrls:{"complete":"https://registry.npmmirror.com/oh-my-live2d/latest/files/lib/complete.js","cubism2":"https://registry.npmmirror.com/oh-my-live2d/latest/files/lib/cubism2.js","cubism5":"https://registry.npmmirror.com/oh-my-live2d/latest/files/lib/cubism5.js"},mobileDisplay:true,models:[{"path":"https://model.oml2d.com/koharu/model.json","mobilePosition":[0,0],"mobileScale":0.1,"mobileStageStyle":{"width":180,"height":166},"motionPreloadStrategy":"IDLE","position":[0,0],"scale":0.12,"stageStyle":{"width":250,"height":250}},{"path":"https://model.oml2d.com/HK416-2-destroy/model.json","scale":0.12,"position":[0,0],"stageStyle":{"width":250},"mobileScale":0.08,"mobilePosition":[0,0],"mobileStageStyle":{"width":180}},{"path":"https://model.oml2d.com/rem_2/model.json","scale":0.12,"position":[0,220],"mobileScale":0.08,"mobilePosition":[-5,5],"mobileStageStyle":{"width":180},"stageStyle":{"width":250}}],parentElement:document.body,primaryColor:"var(--btn-bg)",sayHello:false,tips:{style: {"width":230,"height":120,"left":"calc(50% - 20px)","top":"-100px"},mobileStyle: {"width":180,"height":80,"left":"calc(50% - 30px)","top":"-100px"},idleTips:{interval:15000,message:function(){
  return axios.get('https://v1.hitokoto.cn?c=i')
    .then(function (response) {
      return response.data.hitokoto ;
    })
    .catch(function (error) {
      console.error(error);
    });
}
}}});</script><!-- hexo injector body_end end --></body>
</html>